# -*- coding: utf-8 -*-
"""VOIS_Sentiment_analysis_feedback_addition (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HCKVfUgegnL4QQktpTNtFevfTa8XrjYc
"""

#Step 3: Sentiment Analysis

#Apply NLP sentiment models to classify each feedback.
#Options:

#VADER (quick, rule-based)

#TextBlob (basic polarity)

#Transformers (BERT / DistilBERT) → most impressive

#📌 Deliverable: Dataset with new column → sentiment_label (Positive / Neutral / Negative)

from google.colab import files
import pandas as pd

# Upload file
uploaded = files.upload()

# Load the correct Excel file
df = pd.read_excel("customer_feedback_cleaned.xlsx")

# Show first few rows
df.head()

df.columns

df.columns.tolist()

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd

# Initialize VADER
analyzer = SentimentIntensityAnalyzer()

# Function to get sentiment
def get_vader_sentiment(text):
    score = analyzer.polarity_scores(str(text))['compound']
    if score >= 0.05:
        return "Positive"
    elif score <= -0.05:
        return "Negative"
    else:
        return "Neutral"

# Apply sentiment on the correct column
df["sentiment_vader"] = df["CleanedFeedback"].apply(get_vader_sentiment)

# Preview results
df[["CleanedFeedback", "sentiment_vader"]].head()

!pip install textblob  #TextBlob is a Python tool that helps you understand sentiment in text—whether someone is feeling positive, negative, or neutral.

from textblob import TextBlob

def get_textblob_sentiment(text):
    analysis = TextBlob(str(text))
    polarity = analysis.sentiment.polarity
    if polarity > 0:
        return "Positive"
    elif polarity < 0:
        return "Negative"
    else:
        return "Neutral"

df["sentiment_textblob"] = df["CleanedFeedback"].apply(get_textblob_sentiment)

df[["CleanedFeedback", "sentiment_vader", "sentiment_textblob"]].head()

!pip install transformers
from transformers import pipeline

# Load pre-trained sentiment model
sentiment_pipeline = pipeline("sentiment-analysis")

# Apply to dataset (on first 200 rows for speed, then expand)
df["sentiment_bert"] = df["CleanedFeedback"].astype(str).apply(lambda x: sentiment_pipeline(x[:512])[0]['label'])

df[["CleanedFeedback", "sentiment_bert"]].head()

df["sentiment_vader"].value_counts()
df["sentiment_textblob"].value_counts()
df["sentiment_bert"].value_counts()

df.to_excel("customer_feedback_with_sentiment.xlsx", index=False)

from google.colab import files
files.download("customer_feedback_with_sentiment.xlsx")

#Next step: Topic Modeling (Key Insight Part)

#Extract main topics from feedback.
#Methods:

#LDA (Latent Dirichlet Allocation) → classic

#BERTopic → modern, more accurate

#KMeans clustering with TF-IDF

#📌 Deliverable: List of topics (e.g., Billing Issues, Network Quality, Customer Support).
#Sentiment analysis shows how customers feel; topic modeling reveals what they’re talking about. Together, they help you find real issues—like billing or support—without reading every comment.

import pandas as pd

# Load dataset
df = pd.read_excel("customer_feedback_cleaned.xlsx")

# Check first rows
print(df.head())

# Check column names
print(df.columns)

import pandas as pd

# Load dataset
df = pd.read_excel("customer_feedback_cleaned.xlsx")

# Extract feedback column
texts = df["CleanedFeedback"].astype(str).tolist()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Vectorize
vectorizer = CountVectorizer(stop_words="english", max_df=0.95, min_df=2)
X = vectorizer.fit_transform(texts)

# Train LDA
lda = LatentDirichletAllocation(n_components=5, random_state=42)  # 5 topics
lda.fit(X)

# Show top words for each topic
words = vectorizer.get_feature_names_out()
for idx, topic in enumerate(lda.components_):
    print(f"\nTopic {idx+1}:")
    print([words[i] for i in topic.argsort()[-10:]])

!pip install bertopic

from bertopic import BERTopic

# Train BERTopic
topic_model = BERTopic(language="english")
topics, probs = topic_model.fit_transform(texts)

# Save topics into dataframe
df["Topic_BERTopic"] = topics

# View discovered topics
print(topic_model.get_topic_info().head())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Vectorize with TF-IDF
vectorizer = TfidfVectorizer(stop_words="english")
X = vectorizer.fit_transform(texts)

# Apply KMeans
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
kmeans.fit(X)

# Assign cluster labels
df["Topic_KMeans"] = kmeans.labels_

# Show top terms per cluster
terms = vectorizer.get_feature_names_out()
for i in range(5):  # 5 clusters
    cluster_terms = [terms[ind] for ind in kmeans.cluster_centers_[i].argsort()[-10:]]
    print(f"\nCluster {i}: {cluster_terms}")

df.to_excel("customer_feedback_with_topics.xlsx", index=False)
print("✅ Topics added and saved to Excel")

print(df.columns)

print(df.columns)
# look for 'sentiment', 'sentiment_label', or 'polarity'

!pip install nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk

# Download VADER lexicon (only once)
nltk.download("vader_lexicon")

# Initialize analyzer
sia = SentimentIntensityAnalyzer()

# Apply sentiment scoring
df["sentiment_score"] = df["CleanedFeedback"].apply(lambda x: sia.polarity_scores(str(x))["compound"])

# Convert score → label
df["sentiment_label"] = df["sentiment_score"].apply(
    lambda x: "Positive" if x > 0.05 else ("Negative" if x < -0.05 else "Neutral")
)

import matplotlib.pyplot as plt

# Group by Internet Service type
sentiment_by_internet = df.groupby(["InternetService", "sentiment_label"]).size().unstack().fillna(0)

# Plot
sentiment_by_internet.plot(kind="bar", stacked=True, figsize=(8,5))
plt.title("Sentiment by Internet Service Type")
plt.ylabel("Number of Feedbacks")
plt.show()

#(Segmentation Expansion)

#For PhoneService
sentiment_by_phone = df.groupby(["PhoneService", "sentiment_label"]).size().unstack().fillna(0)
sentiment_by_phone.plot(kind="bar", stacked=True, figsize=(8,5))
plt.title("Sentiment by Phone Service")
plt.ylabel("Number of Feedbacks")
plt.show()

#For Contract
sentiment_by_contract = df.groupby(["Contract", "sentiment_label"]).size().unstack().fillna(0)
sentiment_by_contract.plot(kind="bar", stacked=True, figsize=(8,5))
plt.title("Sentiment by Contract Type")
plt.ylabel("Number of Feedbacks")
plt.show()

#For Region / State
sentiment_by_region = df.groupby(["Region", "sentiment_label"]).size().unstack().fillna(0)
sentiment_by_region.plot(kind="bar", stacked=True, figsize=(10,6))
plt.title("Sentiment by Region")
plt.ylabel("Number of Feedbacks")
plt.show()

sentiment_summary = df['sentiment_label'].value_counts(normalize=True) * 100
print(sentiment_summary)

negative_feedback = df[df['sentiment_label'] == "Negative"]
negative_by_contract = negative_feedback['Contract'].value_counts()
print(negative_by_contract)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# For negative feedback
text_neg = " ".join(negative_feedback["CleanedFeedback"].astype(str))
wc = WordCloud(width=800, height=400, background_color="white").generate(text_neg)

plt.figure(figsize=(10,5))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.title("Most Frequent Words in Negative Feedback")
plt.show()

# 📌 Full Sentiment Analysis Workflow (Corrected & Safe)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# ✅ 1. Sentiment Summary
if "sentiment_label" in df.columns:
    plt.figure(figsize=(6,4))
    sent_counts = df['sentiment_label'].value_counts()
    sns.barplot(x=sent_counts.index, y=sent_counts.values, palette="viridis")
    plt.title("Overall Sentiment Distribution")
    plt.ylabel("Number of Feedbacks")
    plt.show()

    print("Sentiment % Distribution:")
    print(df['sentiment_label'].value_counts(normalize=True) * 100)
else:
    print("⚠️ Column 'sentiment_label' not found in DataFrame")

# ✅ 2. Sentiment by Contract Type
if {"Contract","sentiment_label"}.issubset(df.columns):
    plt.figure(figsize=(8,5))
    sns.countplot(data=df, x="Contract", hue="sentiment_label", palette="Set2")
    plt.title("Sentiment by Contract Type")
    plt.ylabel("Number of Feedbacks")
    plt.xticks(rotation=45)
    plt.show()

# ✅ 3. Sentiment by Payment Method
if {"PaymentMethod","sentiment_label"}.issubset(df.columns):
    plt.figure(figsize=(8,5))
    sns.countplot(data=df, x="PaymentMethod", hue="sentiment_label", palette="coolwarm")
    plt.title("Sentiment by Payment Method")
    plt.ylabel("Number of Feedbacks")
    plt.xticks(rotation=45)
    plt.show()

# ✅ 4. Sentiment by Region
if {"Region","sentiment_label"}.issubset(df.columns):
    plt.figure(figsize=(8,5))
    sns.countplot(data=df, x="Region", hue="sentiment_label", palette="muted")
    plt.title("Sentiment by Region")
    plt.ylabel("Number of Feedbacks")
    plt.xticks(rotation=45)
    plt.show()

# ✅ 5. Word Clouds
if {"sentiment_label","CleanedFeedback"}.issubset(df.columns):
    # Negative Feedback WordCloud
    neg_text = " ".join(df[df['sentiment_label']=="Negative"]["CleanedFeedback"].astype(str))
    if neg_text.strip():
        wc_neg = WordCloud(width=800, height=400, background_color="white", colormap="Reds").generate(neg_text)
        plt.figure(figsize=(10,5))
        plt.imshow(wc_neg, interpolation="bilinear")
        plt.axis("off")
        plt.title("Word Cloud - Negative Feedback", fontsize=14)
        plt.show()
    else:
        print("⚠️ No negative feedbacks found")

    # Positive Feedback WordCloud
    pos_text = " ".join(df[df['sentiment_label']=="Positive"]["CleanedFeedback"].astype(str))
    if pos_text.strip():
        wc_pos = WordCloud(width=800, height=400, background_color="white", colormap="Greens").generate(pos_text)
        plt.figure(figsize=(10,5))
        plt.imshow(wc_pos, interpolation="bilinear")
        plt.axis("off")
        plt.title("Word Cloud - Positive Feedback", fontsize=14)
        plt.show()
    else:
        print("⚠️ No positive feedbacks found")

# ✅ 6. Key Drivers of Negative Feedback
if {"Contract","sentiment_label"}.issubset(df.columns):
    neg_contract = df[df['sentiment_label']=="Negative"]["Contract"].value_counts()
    print("Negative Feedback by Contract:\n", neg_contract)

# ✅ 7. Simple Predictive Model Setup
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

df_model = df.copy()

# Columns we expect to encode
cat_cols = ['gender','Dependents','PhoneService','MultipleLines','InternetService',
            'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',
            'StreamingTV','StreamingMovies','Contract','PaperlessBilling',
            'PaymentMethod','Region','State']

le = LabelEncoder()
for col in (cat_cols + ['sentiment_label']):
    if col in df_model.columns:
        df_model[col] = le.fit_transform(df_model[col].astype(str))

# ✅ Prepare X, y
X = df_model.drop(["CustomerFeedback","CleanedFeedback","sentiment_label"], axis=1, errors="ignore")
y = df_model["sentiment_label"] if "sentiment_label" in df_model.columns else None

# ✅ Drop ID-like columns (unique identifiers not useful for ML)
drop_cols = ["customerID", "CustomerID", "Customer_Id"]
for c in drop_cols:
    if c in X.columns:
        X = X.drop(c, axis=1)

# ✅ Encode any remaining string columns automatically
for col in X.columns:
    if X[col].dtype == "object":
        X[col] = le.fit_transform(X[col].astype(str))

# ✅ Train & Evaluate Model
if y is not None:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("\n📊 Sentiment Prediction Model Report:")
    print(classification_report(y_test, y_pred))
else:
    print("⚠️ 'sentiment_label' not available for model training")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import pandas as pd

# Drop only ID + text columns
drop_cols = ["customerID", "CustomerFeedback", "CleanedFeedback"]
df_ml = df_model.drop(columns=[c for c in drop_cols if c in df_model.columns], errors="ignore")

# Make sure target exists
if "sentiment_label" not in df_ml.columns:
    raise KeyError("⚠️ 'sentiment_label' column not found in df_model")

# Separate features & target
X = df_ml.drop("sentiment_label", axis=1)
y = df_ml["sentiment_label"]

# One-hot encode categorical features
X = pd.get_dummies(X, drop_first=True)

# Ensure all features numeric
X = X.apply(pd.to_numeric, errors="coerce").fillna(0)

print("✅ Feature matrix shape:", X.shape)

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("\n📊 Sentiment Prediction Model Report:")
print(classification_report(y_test, y_pred))

!pip install streamlit plotly wordcloud

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# st.title("Hello Streamlit 🚀")
# st.write("Your app is running on Colab using ngrok.")
#

!ngrok config add-authtoken 31uIpxg5HWHWIMdOxqjxy0x07rW_mgYr3BR6du3LzfRY6cMu

!pkill streamlit
from pyngrok import ngrok
ngrok.kill()

!pkill streamlit

!streamlit run app.py --server.port 8501 &>/dev/null &

from pyngrok import ngrok
ngrok.kill()   # kills old ngrok sessions

public_url = ngrok.connect(8501)
print("✅ Streamlit app is live at:", public_url)

from pyngrok import ngrok

# Kill old tunnels
ngrok.kill()

# Start new tunnel
public_url = ngrok.connect(8501)
print("✅ Streamlit app is live at:", public_url)

!pip install streamlit pyngrok textblob matplotlib scikit-learn

!pip install streamlit pyngrok -q

from pyngrok import ngrok
import subprocess

# 🔑 Add your real ngrok authtoken (you already have it)
!ngrok config add-authtoken 31uIpxg5HWHWIMdOxqjxy0x07rW_mgYr3BR6du3LzfRY6cMu

# 🔹 Create the Streamlit app
app_code = """
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer

st.title("📊 Customer Feedback Sentiment Dashboard")

uploaded_file = st.file_uploader("Upload your CSV", type=["csv"])

if uploaded_file is not None:
    # Load full CSV
    df = pd.read_csv(uploaded_file, encoding="utf-8", on_bad_lines="skip")
    st.write(f"Loaded {df.shape[0]} rows and {df.shape[1]} columns")
    st.dataframe(df)  # Show full dataframe

    # ✅ Check if CustomerFeedback column exists
    if 'CustomerFeedback' not in df.columns:
        st.warning("⚠️ 'CustomerFeedback' column not found. Please check your CSV.")

    # ✅ Let user select which sentiment column to use
    sentiment_columns = [col for col in df.columns if col.lower() in ['sentiment_bert','sentiment_textblob','sentiment_vader']]
    if sentiment_columns:
        selected_sentiment = st.selectbox("Select sentiment column for analysis:", sentiment_columns)
        df["sentiment"] = df[selected_sentiment]  # unify column name to 'sentiment'
        st.success(f"Using '{selected_sentiment}' column for sentiment analysis ✅")

        # Sentiment Distribution Charts
        st.write("### Sentiment Distribution")
        sentiment_counts = df["sentiment"].value_counts()
        st.bar_chart(sentiment_counts)

        fig, ax = plt.subplots()
        ax.pie(sentiment_counts, labels=sentiment_counts.index, autopct="%1.1f%%")
        st.pyplot(fig)

        # Top pain points (negative feedback)
        st.write("### Top Pain Points (Negative Feedback)")
        if 'CustomerFeedback' in df.columns:
            negatives = df[df["sentiment"]=="Negative"]["CustomerFeedback"].dropna().astype(str)
            if not negatives.empty:
                vec = CountVectorizer(stop_words="english", max_features=10)
                bag = vec.fit_transform(negatives)
                top_words = pd.DataFrame({
                    "Word": vec.get_feature_names_out(),
                    "Count": bag.toarray().sum(axis=0)
                })
                st.bar_chart(top_words.set_index("Word"))
            else:
                st.write("No negative feedback found ✅")

            # Positive themes
            st.write("### Positive Themes")
            positives = df[df["sentiment"]=="Positive"]["CustomerFeedback"].dropna().astype(str)
            if not positives.empty:
                vec = CountVectorizer(stop_words="english", max_features=10)
                bag = vec.fit_transform(positives)
                top_words = pd.DataFrame({
                    "Word": vec.get_feature_names_out(),
                    "Count": bag.toarray().sum(axis=0)
                })
                st.bar_chart(top_words.set_index("Word"))
            else:
                st.write("No positive feedback found ✅")
        else:
            st.warning("⚠️ 'CustomerFeedback' column not detected for text analysis.")
    else:
        st.error("❌ No sentiment columns (sentiment_bert, sentiment_textblob, sentiment_vader) found in CSV.")



"""

# 👉 Save into app.py
with open("app.py", "w") as f:
    f.write(app_code)

# 🔹 Run Streamlit on port 8501
port = 8501
public_url = ngrok.connect(port)
print("✅ Streamlit app is live at:", public_url)

# 🔹 Launch Streamlit in background
process = subprocess.Popen(["streamlit", "run", "app.py", "--server.port", str(port)])

!streamlit run app.py & npx localtunnel --port 8501

|from pyngrok import ngrok, conf

# Paste your token here
!ngrok config add-authtoken YOUR_AUTHTOKEN

import streamlit as st
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv("customer_feedback_processed.csv")

# Sidebar filters
st.sidebar.header("Filters")
region = st.sidebar.selectbox("Select Region", df["Region"].unique())
service = st.sidebar.selectbox("Select Service Type", df["InternetService"].unique())

# Filtered dataframe
filtered_df = df[(df["Region"] == region) & (df["InternetService"] == service)]

# Sentiment Pie Chart
sentiment_counts = filtered_df["sentiment_label"].value_counts(normalize=True) * 100
fig = px.pie(values=sentiment_counts.values, names=sentiment_counts.index,
             title="Sentiment Distribution")
st.plotly_chart(fig)

# Negative Sentiment by Contract
negatives = filtered_df[filtered_df["sentiment_label"] == "Negative"]
neg_contract = negatives["Contract"].value_counts()
fig2 = px.bar(x=neg_contract.index, y=neg_contract.values,
              labels={"x": "Contract Type", "y": "Count"},
              title="Negative Feedback by Contract")
st.plotly_chart(fig2)

# Wordcloud
text = " ".join(filtered_df["CleanedFeedback"].astype(str))
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
st.pyplot(plt)

# Show some feedback examples
st.subheader("Sample Feedback")
st.write(filtered_df[["customerID", "CleanedFeedback", "sentiment_label"]].head(10))